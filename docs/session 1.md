# **ğŸš€ Session 1: Introduction to LLMs and APIs**

## **ğŸ¯ Objective**

By the end of this session, students will:

- âœ… Understand how to call an LLM via an API using Groq.
- âœ… Send basic prompts and process responses using LangGraph.
- âœ… Begin brainstorming their final project workflow using LangGraph.

---

## **1ï¸âƒ£ Calling an LLM via API**

### **ğŸ§ Understanding API-Based LLMs**

Unlike running an LLM on a local machine, we will interact with the model through an API. The API processes a request, generates a response, and returns it. This method is efficient and offloads computational complexity to external servers.

### **âš™ï¸ Setting Up**

Before using LangGraph to call an LLM, ensure you have the following:

- ğŸ New virtual environment (see previous documents)
- ğŸ“¦ `langgraph` and `groq` installed via pip:
    
    ```bash
    pip install langgraph langchain-openai langchain
    ```
    
- ğŸ”‘ An API key for Groq.
- Setup the config file, fill in `your key` (keep it secret):
```python
# from langchain_groq import ChatGroq
# import os

# # Set up API key
# os.environ["GROQ_API_KEY"] = "your key"
# llm = ChatGroq(model="llama-3.1-8b-instant")

from langchain_openai import ChatOpenAI
import os

llm = ChatOpenAI(
    api_key="your key",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    model="qwen-turbo",
    # other params...
)
```

> you can also use other llms as long as they support langchain/langgraph (search on the Internet)

### **ğŸ“ Basic Example: Calling an LLM in LangGraph**

Hereâ€™s a simple script to send a prompt and get a response:

```python
# a seperate file from config.py, make sure they are under the same directory
import os
from config import llm

# Invoke the model with a prompt
response = llm.invoke("What is LangGraph?")

print(response)
```

### **ğŸ’¡ Hands-On Activity: Send Your Own Prompt**

#### **ğŸŸ¢ Easy Challenge:**

- Modify the script to accept user input from the command line and pass it to the model.
    
    ```python
    user_prompt = input("Enter your prompt: ")
    response = llm.invoke(user_prompt)
    print("Response:", response)
    ```
    

#### **ğŸ”´ Pro Challenge:**

- ğŸ¤– Build an assistant with conversation memory.
- ğŸ—‚ï¸ Use a dictionary or a simple database to store past interactions.
- ğŸ”„ Modify the script to consider previous responses when generating new ones.

---

## **2ï¸âƒ£ Final Project Brainstorm: Designing a Custom Workflow**

### **ğŸ§  Overview**

Since this course is about building workflows using LangGraph, students should start thinking about the kind of project they want to build.

### **â“ Guiding Questions**

- ğŸ¤” What type of workflow do you want to automate using an LLM?
- ğŸ”„ Will your project involve multi-step interactions?
- ğŸŒ Do you want it to fetch external information (e.g., Wikipedia, APIs)?
- ğŸ—ï¸ Will it involve user personalization?

### **âœï¸ Hands-On Activity: Idea Sketching**

#### **ğŸ“Œ Task:**

- Spend 5-10 minutes sketching a simple workflow idea on paper or in a text editor.
- Identify at least two key interactions the user will have with the system.
- Share ideas with a partner or the group for feedback.

#### **ğŸŸ¢ Easy Challenge:**

- ğŸ“ Define the input and output of your workflow clearly.
- ğŸ”€ Identify one decision point in your workflow (e.g., user response changes the next step).

#### **ğŸ”´ Pro Challenge:**

- ğŸ“Š Create a simple flowchart of your workflow idea.
- ğŸ”— Identify at least one external tool or API you may want to integrate.
- ğŸ› ï¸ Consider an optional advanced feature (e.g., memory to track user progress).
